{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def clean_and_extract_sentences(url, keywords):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Abrufen von {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # parse HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"noscript\"]):\n",
    "        script.extract()\n",
    "\n",
    "    text = soup.get_text(separator=' ')\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text, language='german')\n",
    "\n",
    "    found_data = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_clean = sentence.strip()\n",
    "\n",
    "        # --- FILTER ---\n",
    "\n",
    "        # 1. Word Count (between 5 and 40 words)\n",
    "        word_count = len(sentence_clean.split())\n",
    "        if not (5 <= word_count <= 40):\n",
    "            continue\n",
    "\n",
    "        # 2. No Emojis\n",
    "        if emoji.emoji_count(sentence_clean) > 0:\n",
    "            continue\n",
    "\n",
    "        # 3. Keyword-Check (case-insensitive)\n",
    "        sentence_lower = sentence_clean.lower()\n",
    "\n",
    "        keyword_found = False\n",
    "        for k in keywords:\n",
    "            if k in sentence_lower:\n",
    "                keyword_found = True\n",
    "                break\n",
    "\n",
    "        if keyword_found:\n",
    "            found_data.append({\n",
    "                'Satz': sentence_clean,\n",
    "                'URL': url\n",
    "            })\n",
    "\n",
    "    return found_data\n",
    "\n",
    "# --- KONFIGURATION ---\n",
    "\n",
    "keywords_raw = \"\"\"\n",
    "a+++, a++, a+, abbaubar, abhängig, abfall, abhaengig, aise, aktiv, alternativ, anbau, anerkannt, arm, artenvielfalt, asc, aus der nähe, aus der naehe, ausgleich, ausgeglichen, ausstoß, ausstoss, aufbereit, baum, baeume, bäume, b-corp, bci, behandelt, belassen, besser, better cotton initiative, bio, blue sign, boden, bewusst, carbon, chemie, chemikalie, chemisch, clean, climatepartner, cmia, co2, kompens, conscious, cradle to cradle, demeter, dueng, düng, earth, eco, effizient, effizienz, emission, emitt, enkel, engagement, engagiert, energie, engel, erde, erneuerbar, ethisch, ethical, fair, fckw, fla, fsc, fußabdr, frei, freundlich, fwf, ganzheitlich, gerecht, gepa, gentechni, gespritzt, gift, global organic textile standard, gots, green, gruen, grün, grs, guetezeichen, gütezeichen, gut, hand, haltbar, haltungsform, heimat, hybrid, hydro, igep, impact, einfluss, ivn, klima, kohlendioxid, kohlenstoffdioxid, kompens, konform, kontroll, koralle, kreislauf, kunststoff, label, lachgas, landbau, langlebig, lebensdauer, lebenszyklus, local, lokal, luft, methan, mikroplastik, msc, müll, muell, nachhaltig, nachgewiesen, nachwachsend, natur, natür, natuer, net zero, net-zero, netto-null, netto null, nettonull, ncp, negativ, neutral, nordischer schwan, ocean, oeco, oeko, oekotex, oeko-tex, öko, ökotex, öuz, ok power, organic, ozean, offset, papier, pflanz, planet, plastik, positiv, pvc, pur, rainforest alliance, recycel, recycl, reduz, reduziert, region, rein, repar, ressourcen, risiko, rohstoff, rueckgew, rückgew, sa8000, sauber, schadstoff, schädlich, schaedlich, schon, schmutz, schutz, schuetz, schütz, sdg, self made, selfmade, selbstgemacht, sicher, siegel, solar, sonne, spar, standard, strom, sustainable development goals, sustainab, tier, tierleid, toxisch, tco certified, transport, transformation, transparent, treibhausgas, tüv, umwelt, ursprung, urkunde, verantwort, verpflicht, verarbeit, verpack, vertraeglich, verträglich, vegan, vegetari, von hand, von hier, wald, wasser, wiederverwend, wind, wohlergehen, wwf, wrap, xertifix, zertifi, zeugnis, ziel, zukunft\n",
    "\"\"\"\n",
    "\n",
    "keyword_list = [k.strip().lower() for k in keywords_raw.split(',')]\n",
    "\n",
    "\n",
    "input_csv = 'data/urls.csv'\n",
    "\n",
    "df_urls = pd.read_csv(input_csv, sep=';')\n",
    "\n",
    "urls_to_analyze = df_urls['Seite (URL)'].dropna().unique().tolist()\n",
    "\n",
    "urls_to_analyze = [u for u in urls_to_analyze if str(u).startswith('http')]\n",
    "\n",
    "\n",
    "# --- PROGRAM ---\n",
    "\n",
    "all_results = []\n",
    "\n",
    "print(f\"Starte Analyse von {len(urls_to_analyze)} URLs mit {len(keyword_list)} Keywords...\")\n",
    "\n",
    "for url in urls_to_analyze:\n",
    "    print(f\"Verarbeite: {url}\")\n",
    "    results = clean_and_extract_sentences(url, keyword_list)\n",
    "    all_results.extend(results)\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Export\n",
    "output_filename = \"analyse_ergebnisse.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "\n",
    "print(f\"Fertig! {len(df)} Sätze gefunden. Gespeichert in '{output_filename}'.\")"
   ],
   "id": "755054a0abf16069",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
